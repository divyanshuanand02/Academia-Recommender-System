from titlesim import *
from abstractsim import *
from normalize import *
import time 
import json
import pickle as pl
from math import *

def extract_data():

	top_papers = []

	f = open('./src/AAN/acl-metadata.txt','rb')
	lines=f.readlines()
	# print(lines)
	# print(lines)
	papers=[]
	paper=[]
	for line in lines:
		try:
			line = line.decode()
			if line=='\n' or line=='*\n':
				papers.append(paper)
				
				paper=[]
			else:
				try:
					paper.append(line.split('=')[1].rstrip('}\n').lstrip(' {'))
				except:
					pass
					# print(line.split('='))
		except:
			pass
	return papers

def topic_vetor(abstract):
	documents = []
	documents.append(abstract)
	included_paper = shortlist()
	idx_to_paper = {"abstract" : 0}
	index = 0
	topic_to_idx = {}
	doc_to_topic = {}
	idx_to_topic = {}
	doc_to_topicscore = {}
	for paper in documents:
		n_features = 500
		tf_vectorizer = CountVectorizer(max_features=n_features,
		                                stop_words='english')
		tf = tf_vectorizer.fit_transform(sent_token(paper))
		tf_feature_names = tf_vectorizer.get_feature_names()
		lda = LatentDirichletAllocation(n_components=50, max_iter=5,
		                                learning_method='online',
		                                learning_offset=50.,
		                                random_state=0)
		doc_to_topic[paper] = []
		lda.fit(tf)
		n_top_words = 30
		doc_to_topicscore[paper] = {}
		for topic in lda.components_:
			# topic_to_idx[" ".join([tf_feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]])] = index
			# doc_to_topic[paper].append(" ".join([tf_feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))
			# index += 1
			topic_list = [preprocess(tf_feature_names[i]) for i in topic.argsort()[:-n_top_words-1:-1]]
			topic_score = 0.0
			for i in topic.argsort()[:-n_top_words-1:-1]:
				topic_score += topic[i]

			for wordtopic in topic_list:
				if wordtopic not in topic_to_idx:
					topic_to_idx[wordtopic] = index
					idx_to_topic[index] = wordtopic
					index+=1 
					# print(tf_feature_names[wordtopic])
				doc_to_topic[paper].append(wordtopic)
				if wordtopic not in doc_to_topicscore[paper]:
					doc_to_topicscore[paper][wordtopic] = topic_score

			# topic_orginal = " ".join(topic_list)
			# if topic_orginal not in doc_to_topicscore[paper]:
			# 	doc_to_topicscore[paper][topic_orginal] = topic_score
			# if topic_orginal not in topic_to_idx:
			# 	topic_to_idx[topic_orginal] = index
			# 	index += 1
			# doc_to_topic[paper].append(topic_orginal)
	topic_matrix = np.zeros(len(topic_to_idx))
	for idx,paper in enumerate(documents):
		for topic in topic_to_idx:
			if topic in doc_to_topic[paper]:
				topic_matrix[topic_to_idx[topic]] += doc_to_topicscore[paper][topic]
	return topic_to_idx, idx_to_topic, idx_to_topic, topic_matrix

def top_papers(title,abstract):

	start = time.clock()
	title_scores = titlesim(title)
	print("[ DONE ]\tTitle Similarit\ttime taken : {}".format(time.clock()-start))
	start = time.clock()
	# print("abstract")
	# abs_scores = abstractsim(100)
	f1 = open('./src/idx_to_paper.pkl','rb')
	f2 = open('./src/idx_to_topic.pkl','rb')
	f3 = open('./src/topic_matrix.pkl','rb')

	idx_to_topic, idx_to_paperid, topic_matrix = pl.load(f2), pl.load(f1), pl.load(f3)
	print("Totals number of tags generated by LDA : ",str(topic_matrix.shape))
	topic_matrix = abs_normalize(topic_matrix)

	gtopic_to_idx, gidx_to_topic, gidx_to_paperid, gtopic_matrix = topic_vetor(abstract)


	abs_scores = {}
	gtopic_matrix = gtopic_matrix/np.sqrt(np.sum(gtopic_matrix**2))

	for idx in idx_to_paperid:
		tm = (topic_matrix[idx]/np.sqrt(np.sum(topic_matrix[idx]**2)))
		score = 0.0
		for i,topic in enumerate(tm):
			gtopic = idx_to_topic[i]
			if gtopic in gtopic_to_idx:
				score += gtopic_matrix[gtopic_to_idx[gtopic]]*topic
		# score = (gtopic_matrix/np.sqrt(np.sum(gtopic_matrix**2)))*(topic_matrix[idx]/np.sqrt(np.sum(topic_matrix[idx]**2)))
		# score = np.sum(score)
		abs_scores[idx_to_paperid[idx]] = score
	# print(abs_scores)
	combined_score = [] 
	for paper in title_scores:
		if paper in abs_scores:
			combined_score.append((abs_scores[paper]+title_scores[paper][1],paper,title_scores[paper][0]))
	print("[ DONE ]\tCalculating total score\ttime taken : {}".format(time.clock()-start))
	combined_score.sort(reverse=True)
	# Combined score of all papers retrieved

	"""
		UnComment to print the result (Top 5)
	"""
	# for item in combined_score[-5:]:
	# 	print(item)
	with open('similarity_score.json','w') as result:
		json.dump(combined_score,result,indent=4)

	with open('time.txt','w') as timetaken:
		timetaken.write("timetaken by last execution with time : {} seconds".format(time.clock()-start))

	return idx_to_topic, topic_matrix, idx_to_paperid, combined_score


def active_area(paper_ids,paper_with_scores):
	data = extract_data()

	topics = open('./src/topic_paper.txt','r')
	topicwise_score = {}
	for item in topics:
		paper_id = item.split(',')[0]
		topic = item.split(',')[1]
		for paper in data:
			if (len(paper))==0:
				continue
			if paper_id == paper[0] and paper_id in paper_with_scores:
				try:
					if topic not in topicwise_score:
						topicwise_score[topic] = paper_with_scores[paper_id]/(1+log(2018-int(paper[4])+1))
					else:
						topicwise_score[topic] += paper_with_scores[paper_id]/(1+log(2018-int(paper[4])+1))
					break
				except IndexError:
					if topic not in topicwise_score:
						topicwise_score[topic] = paper_with_scores[paper_id]/(1+log(2018-int(paper[3])+1))
					else:
						topicwise_score[topic] += paper_with_scores[paper_id]/(1+log(2018-int(paper[3])+1))
				except:
					pass
	score_top = []
	summ = 0.0
	for item in topicwise_score:
		score_top.append((topicwise_score[item],item))
		summ += topicwise_score[item]
	summ/= len(score_top)

	bound = 0

	score_top.sort(reverse=True)
	# print(summ)
	# print(score_top[0])

	for idx, item in enumerate(score_top):
		if item[0] < summ:
			bound = idx
			break

	score_top = score_top[:bound]

	return score_top