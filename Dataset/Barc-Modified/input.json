[
	{
		"title":"Chinese unknown word identification as known word tagging",
		"abstract":"This work presents a tagging approach to Chinese unknown word identification based on lexicalized hidden Markov models (LHMMs). In this work, Chinese unknown word identification is represented as a tagging task on a sequence of known words by introducing word-formation patterns and part-of-speech. Based on the lexicalized HMMs, a statistical tagger is further developed to assign each known word an appropriate tag that indicates its pattern in forming a word and the part-of-speech of the formed word. The experimental results on the Peking University corpus indicate that the use of lexicalization technique and the introduction of part-of-speech are helpful to unknown word identification. The experiment on the SIGHAN-PK open test data also shows that our system can achieve state-of-art performance."
	},
	{
		"title":"Feature identification of program source code using regular expression",
		"abstract":"A software application is a collection of various features that are developed to meet the need of particular purposes. To reduce time to develop and to increase software quality, developers reuse similar features from another software. Before reusing the features, developers need to know what are features in the software. The lack or absence of complete documentation may hinder the process of understanding the features. However the application usually comes with the source code. Reading the source code maybe the only option if the documentation is not found. In this paper, we propose a model to reverse engineering a source code to find information about features in software and its dependency. To find features in the source code, we use regular expressions (regex) to find important elements and their dependencies. A call graph is then generated to help understanding these elements. The model has been implemented and have been validated to several case studies. Finding the features in source code depends entirely on the language of the source code. Our research confirms that customizing the pattern in regex easier than scanning and parsing the language syntax to get the features."
	},
	{
		"title":"Semantics and logic of object calculi",
		"abstract":"The main contribution of this paper is a formal characterization of recursive object specifications based on a denotational untyped semantics of the object calculus and the discussion of existence of those (recursive) specifications. The semantics is then applied to prove soundness of a programming logic for the object calculus and to suggest possible extensions. For the purposes of this discussion we use an informal logic of predomains in order to avoid any commitment to a particular syntax of specification logic."
	},
	{
		"title":"Automatic recognition of dialogue acts in complex typology",
		"abstract":"Experiments on automatic recognition of dialogue acts in the Estonian Dialogue Corpus are described. A complex typology consisting of 126 dialogue acts is used. Two different probabilistic suffix tree models have been implemented. The first model uses substrings of utterances in order to classify utterances into several dialogue acts. The second model uses substrings together with discourse information in the form of preceding and following dialogue act. Average recall and precision of 45%-56% have been achieved in the experiments. Patterns for recognition of main information acts are analysed."
	},
	{
		"title":"Experimental Exploration of Support Vector Machine for Cancer Cell Classification",
		"abstract":"text classification is the task of automatically categorizing collections of electronic textual documents into their predefined classes, based on their contents. Due to the increase in the amount of text data in these recent years, document classification has emerged in the form of text classification systems. They have been widely implemented in a large number of applications such as spam filtering, emails, knowledge repositories and ontology mapping. The main essence is to propose a text classification technique based on the feature selection and reduction of the feature vector dimensionality and increase the classification accuracy using pre-processing. This paper gives the detailed study on how support vector machine (SVM) can be used to classify uncertain data. SVM is a powerful and supervised learning sample based on the lowest structural risk principle. During training, this algorithm creates a hyperplane for separating positive and negative samples. The type of kernel used for SVM classifier will be having a major impact on classification results. In this paper Breast Cancer Wisconsin (Diagnostic) Data Sets are used in order to classify using four types of SVM kernel methods such as linear, polynomial, sigmoid and radial. Classification results obtained reveal that radial kernel method is best-suited data sets. In order to measure the suitability of kernel method, various factors are compared from classification results such as accuracy, kappa value, sensitivity, specificity precision etc."
	},
	{
		"title":"Duplicate bug report detection with a combination of information retrieval and topic modeling",
		"abstract":"Detecting duplicate bug reports helps reduce triaging efforts and save time for developers in fixing the same issues. Among several automated detection approaches, text-based information retrieval (IR) approaches have been shown to outperform others in term of both accuracy and time efficiency. However, those IR-based approaches do not detect well the duplicate reports on the same technical issues written in different descriptive terms. This paper introduces DBTM, a duplicate bug report detection approach that takes advantage of both IR-based features and topic-based features. DBTM models a bug report as a textual document describing certain technical issue(s), and models duplicate bug reports as the ones about the same technical issue(s). Trained with historical data including identified duplicate reports, it is able to learn the sets of different terms describing the same technical issues and to detect other not-yet-identified duplicate ones. Our empirical evaluation on real-world systems shows that DBTM improves the state-of-the-art approaches by up to 20% in accuracy."
	},
	{
		"title":"Neural Networks and Deep Learning",
		"abstract":"This chapter contains sections titled: Artificial Neural Networks, Neural Network Learning Algorithms, What a Perceptron Can and Cannot Do, Connectionist Models in Cognitive Science, Neural Networks as a Paradigm for Parallel Processing, Hierarchical Representations in Multiple Layers, Deep Learning"
	},
	{
		"title":"Study on dipping mathematical models for solder Flip-chip bonding in microelectronics packaging",
		"abstract":"In order to develop the flux dipping process, a quantitative mathematical model that accurately describes the flux dipping process in flip-chip bonding is proposed. The whole dynamic dipping process is captured by high-speed transient imaging followed by image processing. Curve-fitting of experimental results yield the relation between flux dipping quantity and process parameters. It suggests a quadratic function of quantity (Q) with respect to dipping depth (d), and a piece-wise function to dipping speed (v) including an exponential section and a quadratic or linear section. While the coupled effect of d and v can be expressed as a merged function of Q=f(d,v), these mathematical models are proved to be effective when applied to actual dipping process practiced on a flip-chip bonding machine, thus providing a reliable mathematical basis for optimizing dipping flux in industrial manufacturing."
	}
]