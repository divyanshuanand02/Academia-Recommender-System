Citation Recommendation without Author Supervision
Qi He†

Daniel Kifer§
†,§

†

Jian Pei‡

C. Lee Giles†

‡

The Pennsylvania State University, Simon Fraser University

{qhe, pmitra, giles}@ist.psu.edu, § dan+wsdm11@cse.psu.edu, ‡ jpei@cs.sfu.ca

ABSTRACT
Automatic recommendation of citations for a manuscript is
highly valuable for scholarly activities since it can substantially improve the efficiency and quality of literature search.
The prior techniques placed a considerable burden on users,
who were required to provide a representative bibliography
or to mark passages where citations are needed. In this
paper we present a system that considerably reduces this
burden: a user simply inputs a query manuscript (without
a bibliography) and our system automatically finds locations where citations are needed. We show that the naı̈ve
approaches do not work well due to massive noise in the document corpus. We produce a successful approach by carefully examining the relevance between segments in a query
manuscript and the representative segments extracted from
a document corpus. An extensive empirical evaluation using
the CiteSeerX data set shows that our approach is effective.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Design, Experimentation

Keywords
Bibliometrics, Context, Extraction, Recommender Systems

1.

Prasenjit Mitra†

INTRODUCTION

One niche in online search is a literature search of academic papers. A standard approach is to perform keyword
searches to retrieve documents, manually review them, and
then follow the chain of bibliographic citations. This is a
labor-intensive process which is especially challenging for
newcomers in a research area. What we would like is a
system that can automatically recommend citations for a

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$10.00.

manuscript that we have written without forcing us to do
most of the work. Such a citation recommendation system
can substantially improve both the effectiveness and efficiency of literature search. For example, the system can
bring up some literatures that the authors are not aware of.
In this paper we present a system that takes a manuscript
(referred to as the query manuscript) as input and automatically identifies where citations are needed and provides
a list of candidate documents to be cited. Unlike previous
work [15, 26, 24, 31, 25, 9], we do not require that the authors of the query manuscript provide the system with a
partial but representative bibliography and we also do not
require them to identify locations where a citation is needed.
Thus we seek to minimize the users’ burden in performing a
literature search.
In our prior work [9], we addressed the issue of recommending documents if we knew which parts of the query
manuscript required citations. In this paper, we build upon
[9] by eliminating the requirement that a user marks passages that require citations. Thus our goal is to automatically analyze a manuscript lacking a bibliography and to
suggest locations where citations are needed. These locations (and the words surrounding them) can also serve as
explanations for why certain citations were recommended.
When combined with the retrieval techniques developed in
our earlier work [9], which take these candidate citation contexts as input, we can achieve a system that automatically
recommends citations for a query manuscript and also identifies where citations should go. This places the least amount
of burden on a user compared to other citation recommendation techniques.
Technically, a citation context in a document d is a snippet of words surrounding a reference to another document
d0 . Such a snippet can be a very accurate description of some
of the ideas and contributions of d0 . In a query manuscript,
a candidate citation context is a passage that seems to describe an idea already present in the literature (and thus
in need of citation). Thus, the essence of our approach is
to generate candidate citation contexts by properly comparing a query manuscript to millions of citation contexts that
can be extracted from a large corpus of documents such as
CiteSeerX.
The naı̈ve approaches do not work well. The comparisons
(e.g., cosine similarity) between text in a query manuscript
and citation contexts extracted from a document corpus produce unreliable results because individual citation contexts
can contain a lot of noise. On the other hand, we have found
that direct comparisons to abstracts and the full-text of doc-

uments in the corpus are also unreliable [9]. Abstracts are
generally vague and, due to space constraints, do not adequately describe the contributions and techniques of a document. The full-text of a document in a corpus usually contains noise in the form of additional information that is irrelevant to the citation recommendation task; this frequently
includes discussions of notation, experimental setups, etc.
The full-text of a document can also contain many ideas but
only a few of them may be relevant to the query manuscript.
Thus the full-text would need to be segmented into homogeneous regions representing these ideas. This makes retrieval
based on full-text similarity very slow and error-prone. In
contrast, we can think of citation contexts as the equivalent
of a homogeneous segment of full-text that describes one
idea.
To the best of our knowledge, this is the first paper that
seeks to identify candidate locations in a query manuscript
where citations are needed, which is a challenging problem.
We propose several techniques of increasing refinement to
identify candidate citation contexts in a query manuscript.
We produce a successful approach for finding candidate citation contexts in a query manuscript by carefully evaluating
the relevance between passages in the query manuscript and
citation contexts extracted from a document corpus. Moreover, our method considers the relationships between passages in the query manuscript to reduce the redundancy in
candidate citation contexts. We thoroughly evaluate these
techniques using the CIteSeerX digital library.
The rest of this paper is organized as follows. We discuss
related work in Section 2. We formalize the problem in Section 3. We discuss methods for generating candidate citation
contexts in Section 4 and report the empirical evaluation in
Section 5. Section 6 concludes the paper.

2.

RELATED WORK

The problem addressed in this paper is to recommend citations for a query manuscript while placing as little burden
as possible on the user and to identify locations in the query
manuscript where references are needed. In contrast to this,
prior work on citation recommendation either required the
user to produce a representative bibliography or to explicitly
mark locations where citations are needed. In this section we
review the related work on citation recommendation as well
as some related work on link prediction, contextual analysis,
and online advertising. Figure 1 illustrates the position of
this paper in literature.
Many proposals for citation recommendation require a
partial list r0 of citations and attempt to recover the full
bibliography r ⊃ r0 . There is an implicit requirement that
r0 is representative of the types of papers that need to be
cited. McNee et al. [15] used a collaborative filtering approach based on various rating matrices including an authorcitation matrix, paper-citation matrix, and a co-citation matrix. Documents which are co-cited often with citations in
r0 are candidates for recommendation. Zhou et al. [31] propagated positive labels ( the existing citations in r0 ) through
multiple graphs such as the paper-paper citation graph, the
author-paper bipartite graph, and the paper-venue bipartite
graph, and learned the labels for documents (relevant/not
relevant) with respect to a given testing document in a semisupervised manner. Torres et al. [26] used a combination
of context-based and collaborative filtering algorithms that
used citations to build a recommendation system, and re-

input
document
[this paper]

citation contexts
recommendation
to authors

partial bibliography

intermediate
results

output

citation contexts

our prior
work [9]

[15,24,25,26,31]

bibliography
+
citations for each
placeholder
bibliography

author supervision
[6,8,11,16,22,25]

recommendation
to readers

other contextual
applications

user profile

documents for
citation/review

[2,5,28]

[4,19,27,29]

keywords/
phrases/entities
hyperlinks/ads for
each placeholder

Figure 1: Position of this paper in literature.

ported that the hybrid algorithms performed better than
individual ones. Strohman et al. [24] experimented with a citation recommendation system where the relevance between
two documents is measured by a linear combination of text
features and citation graph features. They concluded that
similarity between bibliographies of documents and Katz distance [14] are the most important features. Tang and Zhang
[25] explored recommending citations for specific locations
in a query manuscript. However, a user must provide a representative bibliography as this information is used to compute features in the hidden layer of a Restricted Boltzmann
Machine before predictions can be made.
Shaparenko and Joachims [22] proposed a technique to
measure the influence of documents in a corpus on a query
manuscript d. Their technique does not require a partial bibliography. Instead, they use language modeling and convex
optimization to recommend documents. However, this technique does not identify passages where citations are needed.
Scalability is also an issue with this technique and so documents are initially weeded out by textual similarity (either
full-text or abstract) to the query manuscript. Full-text similarity is too slow for large digital libraries, while similarity
based on abstracts yields poor recall [9] (because an abstract
does not contain enough information to identify much of the
related literature).
Topic models [3] provide a low-dimensional representation of documents in terms of automatically discovered and
comprehensible “topics”. They have been extended to incorporate citation information as well and so they can also
be used to recommend citations. The aforementioned work
of Tang and Zhang [25] fits in this framework. Nallapati et
al. [16] introduced a model called Pairwise-Link-LDA which
models the presence or absence of a link between every pair
of documents and thus does not scale to large digital libraries. It also introduced a simpler model that is similar to
the work of Cohn and Hofmann [6] and Erosheva et al. [8].
Here citations are modeled as a sample from a probability
distribution associated with a topic. Thus, a paper can be
associated with topics when it is viewed as a citation. It
can also be associated with topics from the analysis of its
text. However, there is no mechanism to enforce consistency

between the topics assigned in those two ways. Kataria et
al. [11] extended topic models to handle citation contexts
as well and thus have improved capacity for recommending
citations. However, these models cannot identify locations
where citations are needed. In general, topic models require
a long training process using iterative techniques such as
Gibbs Sampling, EM, or variational inference and must be
periodically retrained as new documents are added to the
digital library.
Other studies recommend documents using user profiles
rather than query manuscripts. Basu et al. [2] focused on
recommending conference paper submissions to reviewers
based on paper abstracts and reviewer profiles that were extracted from the Web. This is a specific step in a more general problem known as the Reviewer Assignment Problem,
surveyed by Wang et al. [28]. Chandrasekaran et al. [5] presented a technique to recommend technical papers to readers
whose profile information is stored in CiteSeer. Publication
records are used to model a user’s profile. User profiles and
documents are presented as hierarchical concept trees with
predefined concepts from the ACM Computing Classification System. The similarity between a user profile and a
document is measured by the weighed tree edit distance.
Studies that use citation contexts to analyze papers have
motivated our work. Previous studies have clearly indicated
that citation contexts are a good summary of the contributions of a cited paper. Huang et al. [10] observed that
citation contexts can effectively help to avoid “topic drifting” in clustering citations into topics. Aya et al. [1] noted
that citation contexts can reflect the information needs (i.e.,
motivations) of citations and built a machine learning algorithm to automatically learn the motivation functions (e.g.,
compare, contrast, use of the citation, etc.) of citations by
using features of citation contexts. Ritchie [21] extensively
examined the impact of various citation context extraction
methods on the performance of information retrieval tasks.
In [9], we studied how to use citation contexts to recommend papers to cite. That work assumed that a user has
provided a query manuscript (without bibliography) that
indicates locations where citations are needed. Then, we
designed a non-parametric probabilistic model that uses information about citation contexts to measure the relevance
between two documents and also between a citation context
and a document. Requiring a user to explicitly mark locations where references are needed places a burden on a user
who may not be familiar with the literature and who may
therefore fail to mark certain locations. Thus we seek to
address this issue in the paper.
Automatically extracting citation contexts is novel approach for citation recommendation and it bears some resemblance to other domains such as hyperlink generation
and content-based advertising. We only mention a few representative papers here to highlight the significant differences
between these tasks and citation recommendation. For example, Brzeski et al. [27] used named entities in a document
as hyperlink snippet candidates. However, in citation recommendation, the ideas that need to be cited are more complicated than named entities. Indeed, if a query manuscript
contains named entities, they usually refer to established
ideas in the literature that the author is already aware of;
marking named entities as items that need to be cited would
not provide added value to a user.
Ad placement (recommending ads for webpage) typically

tries to match ad keywords to a given webpage. For example, Ribeiro-Neto et al. [19] explore techniques using external information to reduce the mismatch in vocabularies of ads and webpages. Broder et al. [4] compared each
section of a Web page to the advertisements by enhancing their relevance at the taxonomy level, thus each section
can be seen as an advertising context candidate. Yih et
al. [29] automatically extracted keywords from a free text
and associated them with advertisements. Keywords are
learned using the machine learning based on a bunch of
linguistic/syntactic/semantic/proximity features from text
and query logs. Even though one may metaphorically view
a webpage as a query manuscript and an ad as a potential paper to cite, those techniques are generally not applicable to citation recommendation. First, ads are designed
to have succinct descriptions while research papers contain
many different ideas that are not described as succinctly;
thus the main objects involved have different characteristics. Second, the types of external datasets that are useful
are also significantly different (for example query logs vs.
digital libraries).

3.

PROBLEM SETUP

In this section, we introduce notation and terminology,
and describe the citation recommendation problem.
We let d denote a document, which is a sequence of words.
A document cites other documents r1 , . . . , rm . A citation
location ` is a place in document d where some document
r is cited as a reference. The citation context c` is the
sequence of words surrounding this citation location ` and
which describes the relevance of r to document d. We say
that c` is an out-link citation context with respect to
d because it describes an outgoing link from document d.
Similarly, we say that c` is an in-link citation context
with respect to document r because it describes a link going
into r.
Given a citation location, identifying the set of words that
describes why reference r is relevant to document d is a challenging task and is left for future work. For the current paper, we thus use a simple but efficient heuristic: if we are
given a citation location `, then the citation context c` consists of the 100 words surrounding `. Our techniques are
designed to be robust to the noise introduced by this heuristic (or any other technique that delimits the citation context
from the rest of the text). Using this heuristic, we can efficiently collect citation contexts from large online document
repositories such as CiteSeerX.
The problem we are addressing is the following. A user
submits an unlabeled query manuscript. This document contains no bibliography and no references. This document
does not even contain markers indicating where references
should go. Our goal is to segment the document into a sequence < w1 , . . . , wn > of possibly overlapping windows and
to label each window wi as either (1) a citation context or
(2) not a citation context. Overlapping windows that are
labeled as citation contexts are merged together. This results in a sequence of disjoint candidate citation contexts
(d)
(d)
< c1 , . . . , ck1 > (we use the term candidate to emphasize
the fact that these are predictions). We may optionally want
(d)
to assign a weight yi to each ci to indicate our confidence.
Once we have discovered the candidate citation contexts
(d)
(d)
c1 , . . . , ck1 (and confidence weights y1 , . . . , yk1 ), we can use

a large online document corpus D (such as CiteSeerX) to
retrieve a ranked list of relevant documents r1 , r2 , . . . . The
relevance sim(d, r) of a document r ∈ D with in-link cita(r)
(r)
tion contexts c1 , . . . , ck2 can be computed using the trace
operator [20] relevance scoring developed in [9]:

sim(d, r) =

k1 k2
1 X X  (d) (r) 2
yi ci · cj
k1 k2 i=1 j=1

(1)

where each citation context is treated as a tf-idf word vector.
If no weights yi are given, we can set them to 1 in Equation
1. Note that the relevance of r to the query manuscript
d is dependent on the similarity of the discussion in d to
what other authors say about r. In practice, we add both
document titles as citation contexts in Equation 1. We found
that this overall approach outperforms textual similarity to
document r [9].

4.

FINDING CITATION CONTEXTS

In this section we propose 4 different models for finding ci(d)
(d)
tation contexts < c1 , . . . , ck > and associated confidence
weights < y1 , . . . , yk > in an unlabeled query manuscript d.
We start with simple baselines and progress towards more
sophisticated use and generation of features culminating in
our proposed approach in Section 4.4.

4.1

Method 1: Language Models

It is quite natural to try to use language models [17] for
finding citation contexts. From CiteSeerX, we can extract
100 words around each citation that occurs in a document
in this corpus. Treating these as positive examples of citation contexts, we get roughly 4 million positive training
examples. In theory, we could use the remaining text in
the corpus as the negative examples. However, this set of
negative examples is too large for us to process and so we
treat this as a one-class classification problem by building
the language models only from the positive examples. Note
that the positive examples are themselves noisy – not all
words surrounding a citation refer to that citation (identifying which words are relevant to a citation is a difficult task
to be considered in future work).
After building an n-gram language model LM1 , we can
use it to try to recognize locations in the query manuscript d
where citations should exist. We can do this with a two-step
approach of scoring windows followed by burst-detection.
First, we divide the query manuscript into a sequence
< w1 , . . . , wn > of overlapping windows of 100 words each.
For example, w1 would contain words 1 through 100, w2
would contain words 2 through 101, w3 would contain words
3 through 102, etc. Using the language model LM, we can
assign a probability score pi to each window wi . This gives
a sequence < p1 , . . . , pn > of scores. We now can use a burst
detection method, such as the 2-state automaton model proposed by Kleinberg [13], to identify regions of relatively high
probability. Each such connected region is output as a candi(d)
date citation context ci with corresponding weight yi given
by the burst detection method [13].

4.2

Method 2: Contextual Similarity

1
Our experiments used the Katz back-off strategy when estimating conditional probabilities [12].

An alternative to language models is to directly compare
similarity between different pieces of text using tf-idf weighting. There are two possibilities here. The first approach is
to divide a document into a sequence < w1 , . . . , wn > of
overlapping windows (as in Section 4.1) and, for each wi , to
compute its similarity to the full text or abstract of documents in the corpus D. In prior work [9], we showed that
this approach leads to poor retrieval of relevant documents.
One of the reasons for this is that the full text or abstract
of a document r ∈ D is not necessarily the best description of the ideas in r. For example, the full-text of a document contains many parts that do not clearly describe or are
marginally relevant to its main contribution. This can include experimental setup, discussion of notation, narratives
containing specific examples, etc. On the other hand, many
abstracts do not adequately describe all aspects of a paper’s
contribution.
In general, we have found that citation contexts that surround references to a document r do a much better job of
describing the important aspects of document r. Indeed,
although a single citation context is noisy, the set of all inlink citation contexts that reference r does contain quite a
bit of signal. Thus a better approach is to compare a window in a query document d to citation contexts extracted
from an online corpus such as CiteSeerX (in fact, this was
the motivation for our initial line of research [9]).
We can use the relevance model based on trace operators
[9] to compute the relevance of window wi to the in-link
(r)
(r)
citation contexts < c1 , . . . , ck2 > for a document r in the
(r)

corpus D (where wi and the cj are treated as tf-idf vectors):
sim(wi , r) =

k2
2
1 X
(r)
wi · cj
k2 j=1

andPwe can assign a score to wi as either maxr∈D sim(wi , r)
or r∈D sim(wi , r)/|D|. We have found that the latter (averaging) approach works better to assign a score for wi .
Thus after we divide a query manuscript d into overlapping windows < w1 , . . . , wn > (as in Section 4.1), we use
P
r∈D sim(wi , r)/|D| to assign a score pi for each window
wi . We again use a burst-detection method [13] on the sequence < p1 , . . . , pn > of scores to find regions with relatively high scores. Each such region is composed of consecutive words from d and is output as a candidate citation
(d)
(d)
context ci . The weight yi assigned to context ci is the
weight given to the associated burst [13].

4.3

Method 3: Topical Relevance

One of the problems with the contextual relevance approach from Section 4.2 is that it is very time-consuming to
compare every window wi with all of the contexts extracted
from a large document corpus. For example, in our CiteSeerX dataset, there are roughly 4 million such contexts to
compare to. Thus dimensionality reduction is needed.
If we treat citation contexts as one type of object and the
documents they refer to as another type of object, we get a
large context-document bipartite graph. The goal is to cluster this bipartite graph to obtain clusters of citation contexts
and clusters of documents. Then we must judiciously choose
representatives for each cluster.
The clustering itself could be performed using, for example, a variation of topic models [3] or co-clustering. Since our

dataset is very large, we did not find significant differences in
quality and even the following approach worked well: first,
cluster the citation contexts (treating each context as a bag
of words), and then separately cluster the documents using
their titles and abstracts.
Given citation context clusters C1 , . . . , Ck1 and document
clusters Z1 , . . . , Zk2 , we can form cluster representatives as
follows. For a citation context cluster Ci , its representative
c̃i is:
1 X
x
c̃i =
|Ci | x∈C
i

which is the average tf-idf vector representing each context in
the cluster Ci . We could, in principle, form representatives
z̃i for the document clusters Zi in the same way. However,
this would represent a document cluster as the average of
the abstracts (treated as tf-idf vectors) in the cluster. As we
noted previously, citations refering to a document can do a
better job than the abstract of describing a document’s relevance. Thus generating document cluster representatives z̃i
is more involved. First, we estimate conditional probabilities
between clusters:
# contexts in Cj citing documents in Zi
;
p(Cj |Zi ) =
total # contexts citing documents in Zi
and then we compute the representative z̃i for cluster Zi as:
z̃i =

k1
X

We thus use what other authors say about documents to
form the cluster representatives. Now we split the query document d into a sequence of overlapping windows < w1 , . . . , wn >
and, using the probabilistic relevance model [9] based on
trace operators [20], we compute a score pi for each wi as:
k2
1 X
(wi · z̃j )2 .
k2 j=1

Once again we use a burst detection method [13] on the
sequence < p1 , . . . , pn > of scores to find regions with relatively high scores. Each such region is composed of consecutive words from d and is output as a candidate citation
(d)
(d)
context ci . The weight yi assigned to context ci is the
weight given to the associated burst [13].

4.4

Both of these issues require additional features that are
combined in nonlinear ways. Some of these features represent long-range dependencies in the query manuscript. We
discuss these features in Section 4.4.1, we discuss the model
incorporating these dependencies in Section 4.4.2, and we
turn this into an ensemble that votes using a Beta-Binomial
noise model in Section 4.4.3

p(Cj | Zi )c̃j .

j=1

pi =

query manuscript frequently mentions “mixtures of multinomial distributions”. Many parts of this document are
therefore related to papers discussing discrete mixture
models and these parts will be given high scores. All
of these parts would be returned in a list of the most
promising candidate citation contexts (even though this
is redundant from the user’s point of view). On the other
hand, other parts of the query document may be related
to different research areas; these parts would receive lower
scores and therefore would not be returned in a list of the
most promising candidate citation contexts (they would
be crowded out by the “mixtures of multinomial distributions”). Thus we would like to return one candidate
context representing the relationship between “mixtures
of multinomial distributions” and the literature, and we
would like to return other candidate citation contexts that
represent different themes. It is interesting to note that
actual citations also follow this pattern – a citation is often
cited at the frontal references if it is mentioned multiple
times in query manuscript.

Method 4: Dependency Feature Model

Now we present our final model which is based on ideas
from Sections 4.1, 4.2, and 4.3 and addresses their deficiencies. The model described in this section is based on the
following two observations:
– The methods in previous sections use a single score to
discriminate between contexts and non-contexts. We have
found that other types of features are also correlated the
presence or absence of contexts, but this correlation is
nonlinear. These features are described in Section 4.4.1.
– The methods in previous sections do not relate a window wi with prior windows in the query manuscript. We
have found that it is important to consider these relations. This is necessary to eliminate redundancy and to
prevent a single theme from dominating the other themes
present in a query manuscript. To see this, suppose a

4.4.1

Additional Features

Table 1 summarizes the features we compute for every
window wi from the sequence of overlapping windows <
w1 , . . . , wn > constructed from the query manuscript d. The
first group of features compares wi to the citation contexts
culled from a corpus D (such as CiteSeerX). Many of them
use the cluster representatives z̃j defined in Section 4.3.
These features include average similarity to all cluster representatives, to the top K representatives, and statistics about
n-grams that are shared between wi and the citation contexts from corpus D. The second group of features compares
wi to the query document d. They range from simple cosine
similarity between wi and d to dependence on prior predictions such as maximum similarity between wi and a previous
(candidate) citation context in d.

4.4.2

Inference over long-range dependencies

For each window wi , let `i be its binary label (1 for citation
context and 0 for non-context). For query manuscript d,
document corpus D, and parameter set Θ (to be specified
later), the learning model would be:
Θ = argmaxΘ p(`1 , . . . , `n |d, D, Θ)
X
= argmaxΘ
log p(`i |{wj : `j = 1 ∧ j < i}, d, D, Θ) (2)
i

since the label `i of window i depends on the previously
predicated citation contexts because of features 14, 15, 16,
and 17. Since the length of the history is not bounded (i.e.
the model is not Markovian), exact inference is intractable.
There are two options: to approximate the model (by adding
a Markovian structure) or to approximate the inference. We
note that the long-range history is important – if a passage towards the end of the query manuscript is similar to
a passage towards the beginning, we should only return the

Type

Feature of window wi
1. sumsim2topic
2. sumtopksim2topic
3. maxsim2topic
4. avgnumtopic
5. avgtopicentropy

Context
features

Long-range
dependencies with
query
manuscript

6. mintopicentropy
7. maxtopicentropy
8. numterm
9. avgglobalfreq
10. numfirstterm
11. sim2doc
12. avglocalfreq
13. avetfidf
14. depnumfirstterm
15. maxsim2context
16. avgsim2context
17. isfirsttopic

Description
Pk2
2
j=1 (wi · z̃j ) (using cluster representatives z̃j as defined in Section 4.3).
(wi · z̃j1 )2 + · · · + (wi · z̃jK )2 for the top K cluster representatives most similar to wi .
maxj (wi · z̃j )2
Avg (over n-grams common to wi and contexts in D) # of z̃j containing the n-gram.
Avg (over n-grams common to wi and contexts in D) of the entropy of tf-idf scores assigned by
the zj to the n-gram.
Similar to avgtopicentropy (Feature 5) but using min instead of avg.
Similar to avgtopicentropy (Feature 5) but using max instead of avg.
# of distinct n-grams common to wi and contexts from D.
Avg (over n-grams common to wi and contexts in D) # of contexts from D containing the n-gram.
# of n-grams in wi that have not appeared in prior windows wj (j < i) from query manuscript d.
Cosine similarity of wi to abstract and title of d.
Avg (over n-grams in wi ) frequency of the n-gram in query manuscript d.
Avg (over n-grams common to wi and contexts from D) TF-IDF score (TF: over d; IDF: over D).
# of n-grams in wi that have not appeared in prior (candidate) contexts from query manuscript d.
Max cosine similarity of wi to prior (candidate) citation contexts in d.
Avg (over prior candidate contexts) cosine similarity of wi to all prior (candidate) contexts in d.
Is argmaxj (wi · zj )2 equal to argmaxj (ŵ · zj )2 for any prior (candidate) context in d?

Table 1: Features for window wi from query manuscript d computed with corpus D.
latter passage as a candidate citation context (to avoid redundancy); in fact, in actual documents the earlier passage
is more likely to contain a reference than the passage that
follows later (thus actual citation patterns are aligned with
our goal of reducing redundancy in the candidate citation
contexts.). Thus a Markovian assumption would not be justified and we choose to approximate the inference instead.
In order to combine features in a nonlinear way, we use
an ensemble of decision trees as our base model in Equation
2 (ensemble creation is discussed in Section 4.4.3). Since
actual citation patterns are aligned with our goal (reducing
redundancy in the output), model training is very easy: we
take a subset of documents from the corpus D, divide each
d ∈ D into a sequence of windows < w1 , . . . , wnd > and
use the true citation information to assign a label to each
window. Thus in training mode, all of the features can be
computed and the learning algorithm for each tree in the
ensemble is unchanged.
Inference, however, can be problematic because in Equation 2 the label of a window wi could depend on windows
wj that are arbitrarily far away. We have found that the
following one-pass greedy approach is efficient, simple, and
works reasonably well: we start predicting from left to right,
and once a window wi is labeled, we do not revisit (change)
this label later; a window wi is labeled as a candidate citation context if its score (given by the ensemble) exceeds a
threshold τ that is set by cross-validation.

4.4.3

Ensemble Voting

Since decision trees do not have algorithmical stability
[18], we use an ensemble of n decision trees. The individual
trees are constructed
– using all of the features from Table 1;
– using only the small set of best features;
– using all but one feature;
– using bootstrapping.
For a window w, each tree Ti returns a prediction saying that leaf making the prediction has mi out of ki training points labeled as citation contexts. We aggregate these
scores using the following approach. There is a p corresponding to the probability that w is a citation context. The mi ,
ki values for each tree are generated from a Binomial(pi , ki )

distribution, where pi is a noisy version of p and mi is the
number of heads that occur in ki coin tosses.
The noisy pi comes from a Beta(γp, γ(1 − p)) distribution
with mean p and variance p(1 − p)/(γ + 1); we can think
of γ as a tuning parameter that affects the variation of the
unobserved pi values. Under this noise model, mi /ki is an
unbiased estimator of p (however the variance of the estimator depends on p and γ). A convex combination of the mi /ki
is also an unbiased estimator of p, and so we would like the
convex combination with minimum variance. Surprisingly,
even though the variance of the estimator depends on the
unknown quantity p, we do not need to know p to form the
estimator. The combined probability estimate is shown in
the following lemma, whose proof is in Appendix A.
Lemma 4.1. The minimum variance unbiased estimator
of p formed as a convex combination of the mi /ki is:
1

n
X

kj
j=1 kj +γ i=1

Pn

5.
5.1

mi
.
ki + γ

(3)

EXPERIMENTS
Experimental Setup

We implemented the citation recommendation system in
the CiteSeerX to evaluate its performance. All research papers that were published and crawled before year 2008 were
used as the training document corpus D. After removing
duplicates and papers for which there were no citation contexts, titles, or abstracts, we obtained 731, 535 unique citing
documents and 528, 647 unique cited documents in the corpus. For each paper, we extracted its title and abstract as
the text content. Within each paper in the corpus, the 50
words before and 50 words after each citation reference were
treated as the corresponding citation context (a discussion
on the number of words to use can be found in [21]). We also
removed stop words from the contexts. In order to preserve
the time-sensitive past/present/future tenses of verbs and
the singular/plural styles of named entities, no stemming
was done, but all words were transferred to lower-case. We
obtained 4, 175, 615 unique citation contexts and 869, 427
unique word terms (unigrams). Bigrams and trigrams were
computed using only the top 65, 535 unigrams because of

Feature removed

sparsity2 . This resulted in 597, 258 bigrams and 188, 154
trigrams. The combination of unigrams and bigrams is used
as the term corpus by default.
We used one set of 100 papers for decision tree construction and parameter tuning. We used another set of 100
papers from early 2008 for testing the quality of candidate
citation contexts output by each method under evaluation.
We implemented all algorithms in C++ and ran them on
a PSU Linux cluster with 8 nodes, each of which has 8 CPU
processors of 2.67GHz and 128G main memory.

5.2

avgsim2context
numfirstterm
maxsim2topic
depnumfirstterm
sumsim2topic
avetopicentropy
sumtopksim2topic
avenumtopic
maxtopicentropy
avelocalfreq
aveglobalfreq
mintopicentropy
numterm
avetfidf
maxsim2context
isfirsttopic
sim2doc

Methods Being Compared

In order to fairly compare different methods for generating candidate citation contexts, we define the concept of
context coverage. The context coverage is the fraction of
the words in a document d that are inside a candidate citation context. Comparisons between different methods can
only be fair if all of the methods generate candidate citation
contexts with similar context coverage. The first 3 methods
we compare are simple baselines.
– Ground truth: This method outputs the 100 words surrounding the actual citation locations. Since we evaluate
all methods on how well their candidate citation contexts
can be used to retrieve relevant documents, this baseline
represents an upper bound on the quality that can be
achieved. The context coverage of ground truth contexts
over the full-text of a testing paper is 34.5%3 . All ground
truth contexts ci are assigned the same weight yi = 1.0 in
Equation 1 to rank relevant documents from corpus D.
– Random: Candidate citation contexts are generated randomly until they achieve a coverage of 0.345. Results are
averaged over 10 runs. We use the weight yi = 1 for each
candidate citation context ci in Equation 1.
– Fulltext: There is only one candidate citation context
but it consists of the whole text of a testing paper; the
context coverage is 1.0.
The previous 3 baselines are used as sanity checks when
evaluating the performance of the models presented in this
paper. We also compare the following methods, which we
tune to get a context coverage close to 0.345.
– Language Models (LM): We build three language models as described in Section 4.1. The first language model
is based on unigrams, the second is based on bigrams, and
the third is based on trigrams.
– Contextual Similarity: Here we use the method described in Section 4.2 that compares a window wi to the citation contexts extracted from the corpus D (CiteSeerX).
– Topical Relevance: This is the method described in
Section 4.3. Citation contexts and documents were clustered into 1, 000 clusters each using top-down bisection
clustering [30].
– Dependency Feature Model: This is the method described in Section 4.4. We used C4.5 [18] to build each
decision tree in the ensemble. We considered the effects
of leaving out one feature on predicting citation contexts.
Table 2 shows the change in accuracy when the corresponding feature is removed; negative numbers mean the
2

Unigrams with a document frequency < 8 are then cut.
On average, there are 24.19 citation contexts in a testing
paper, but they contain considerable overlap; the average
length of a testing paper is 4402.7 words.
3

change in crossvalidation accuracy
-14.68%
-8.58%
-2.44%
-2.44%
-2.02%
-0.42%
-0.42%
-0.41%
0.41%
+1.21%
+4.92%
+5.74%
+7.28%
+7.31%
+7.40%
+9.78%
+13.44%

Table 2: Evaluation of features.
error using all features is 24.5%.

Cross-validation

accuracy dropped, positive numbers mean the accuracy
increased (however, we have found that removing all features with a positive difference causes accuracy to decrease). We see that some of the most important features are the ones incorporating long-range dependencies
between predicted labels (these include avgsim2context
and depnumfirstterm). The topical relevance features are
also considerably important as maxsim2topic and sumsim2topic are ranked among top 5 features. We used an
ensemble of 23 decision trees: 1 decision tree using all
features, 1 decision tree using the top 5 features from Table 2, 17 take-one-feature-out decision trees, and 4 bootstrapped decision trees. We set γ to be 100 and τ to be
0.85 with cross-validation. τ is used to control the context coverage and application-dependent. We conducted
t-tests with hypothesis that a close γ value (against the
best γ = 100 achieved with cross-validation) does not
change vote scores significantly. The p-values for t-tests
are 0.247 (γ=10), 0.965 (γ=1,000), 0.512 (γ=10,000) and
0.442 (γ=100,000) respectively. That is to say, γ has a
wide setting range (the vote scores do not deviate much
from the best ones) and is easy to be tuned in practice.

5.3

Evaluation of Candidate Citation Context
Locations

In this section, we evaluate how well the candidate citation
contexts (generated by different techniques) correspond to
the actual locations where citation are needed. In Section
5.4 we will then evaluate how the quality of recommended
citations that are computed based on the candidate citation
contexts. To perform the evaluation, we identify the true
citation locations in each of our testing documents and use
the 100 words surrounding each location as the ground truth.

5.3.1

Metrics

Let Wg be the part of a testing paper covered by the
ground truth contexts and Wt be the part covered by the
candidate citation contexts generated
by one of the methods
T
discussed earlier. Then Wt Wg measures the correctness

method
Ground Truth
Fulltext (coverage: 1)
Dependency Model
Topical Relevance
Contextual Similarity
Random
Trigram LM
Bigram LM
Unigram LM

CP
1
0.345
0.472
0.427
0.34
0.334
0.273
0.272
0.221

CR
1
1
0.561
0.46
0.355
0.356
0.268
0.268
0.208

PN ratio
+∞
1/1.9
1/1.12
1/1.34
1/1.94
1/2
1/2.66
1/2.68
1/3.52

Table 3: Evaluation of citation locations under the
default context coverage of 0.345.
of Wt while Wt \ Wg measures incorrect predictions. We use
these quantities to define the following 3 metrics:
T
|Wt Wg |
,
|Wt |
T
|Wt Wg |
,
|Wg |

5.4

– Coverage Precision (CP):
– Coverage Recall (CR):

– Positive vs. Negative ratio (PN ratio):

T
|Wg Wt |
.
|Wt \Wg |

As discussed in Section 5.2, we try to keep the context coverage constant in order to make apples-to-apples comparisons
between different methods. That is, the part Wt of a document that is covered by the candidate citation contexts is
constrained to be roughly 0.345 of the total document; this
is the same fraction in the documents (in our test set) which
is covered by the ground truth Wg . For this reason, coverage
precision and coverage recall are not necessarily in conflict
(in contrast to ordinary precision and recall measures used
for other information retrieval tasks).

5.3.2

presence of long-range dependency features that eliminate
redundancy (as discussed in Section 4.4).
The unigram, bigram, and trigram models are extremely
fast, as is random guessing. The running time of contextual
similarity, on the other hand, was over 10 minutes in our
experiments because each window wi needs to be compared
to over half a million documents in our corpus. The topical
relevance and dependency feature approaches are versions of
contextual similarity that use dimensionality reduction and
so are much faster. Topical relevance required approximatley 0.5 seconds and the dependency feature model required
about 1 second. Both running times are acceptable for this
citation recommendation task considering the quality of the
generated candidate citation contexts. The dependency feature model is the more expensive of the two because of the
additional features it computes.

Result Analyses

Table 3 compares baselines and the different methods for
generating candidate citation contexts. We see that the language models do not perform well for this task; in fact, they
are outperformed by a random selection of citation contexts.
It is not surprising that language models did not perform
well – they were only trained on positive examples (citation contexts) but not on negative examples (because it was
infeasible) and they did not have long-range dependencies
built in. However, it was still surprising that they performed
worse than random guessing.
While the unigram language model performed the worst,
the bigram and trigram models had very similar performance. This provides evidence for the notion that pairs
of words are commonly used to refer to established ideas
(e.g., “mixture models”, “information retrieval”, etc.) while
trigrams (e.g., “support vector machines”) are not nearly as
common (in a relative sense).
We see that contextual similarity (Section 4.2) has similar
performance to random guessing. This implies that citation
contexts are very noisy and that a dimensionality reduction
approach (the methods in Section 4.3 and 4.4) would be useful for extracting signal from the noise. This intuition is also
confirmed by Table 3, where the topical relevance method
(Section 4.3) and the dependency feature model (Section
4.4) are the only methods which significantly outperform
random guessing.
We can also see that the dependency feature model outperforms topical relevance as well, and this is due to the

Evaluating Recommended Citations

One difficulty with evaluating how well candidate citation
contexts cover the true citation locations is that there are
many places where citations could have been (but were not)
placed in a research paper (which is being used as the ground
truth). This can happen when a similar idea had been previously referenced, when an idea is very common (e.g., few
people cite Newton or Liebniz when the use derivatives), or
when an author is unaware of related work. Thus in this
section we evaluate the quality of candidate citation contexts differently from Section 5.3. Here we evaluate how
well these candidate citation contexts can be used to recommend papers to cite. To generate recommendations from the
candidate citation contexts, we use the relevance model [9]
which ranks citations using Equation 1 from Section 3.

5.4.1

Metrics

Recall (R): The average fraction of a testing document’s
bibliography that appears in top-k recommended citations.
Relative co-cited probability (P): Some papers that
are recommended may be relevant even if they are not cited
by the testing document. This can occur because an author is unaware of related work or due to space constraints.
Because of this, measures such as precision relative to the
actual bibliography are not as meaningful. Thus in this paper, we will use a modified measure of accuracy based on
the assumption that papers that are frequently co-cited are
relevant to each other. Let d be a testing document, bi a
document cited by d, and rj a document that was recommended but not cited by d. We begin by computing the
following quantity:
p(rj | bi ) =

number of papers in D citing both bi and rj
.
number of papers in D citing bi

For each testing document, we average p(rj | bi ) over all
< rj , bi > pairs and then we average over all testing documents. The result is the relative co-cited probability.
NDCG: The effectiveness of a recommendation system is
also sensitive to the ordering of the relevant recommended
citations (i.e., the most relevant citations should be highly
ranked). This is not directly measured by the previous two
metrics, and so we also use the normalized discounted cumulative gain (NDCG) to measure the ranking. The NDCG

value of a ranked list at position i is calculated as:
N DCG@i = Zi

i
X
2r(j) − 1
,
log(1 + j)
j=1

where r(j) is the rating of the j-th document in the ranking
list, and the normalization constant Zi is chosen so that the
perfect list gets a NDCG score of 1. Given a testing document d and any other document r from our corpus D, we use
the average relative co-cited probability of r with all original
citations of d to weigh the citation relevance score of r to d.
Then we sort the documents in D with respect to this score
(relevance to d) and assign a new score (5 point scale) to each
document. If Rmax is the maximum relevance of any r ∈ D
to d, then documents are assigned the scores 4, 3, 2, 1, 0 if
their relevance to d falls within the ranges (3Rmax /4, Rmax ],
(Rmax /2, 3Rmax /4], (Rmax /4, Rmax /2], (0, Rmax /4], and 0,
respectively. We report the average NDCG score over all
testing documents.

5.4.2

Result Analyses

Figure 2 shows the citation recommendation performances
of the baselines and methods for generating candidate citation contexts. Comparing with Table 3, we see that recommendation quality is correlated with the ability to cover the
ground truth citation locations.
One exception is the baseline of guessing candidate citation contexts randomly. Its coverage precision/recall are
not high but it is able to generate a decent bibliography
anyway. Initially it may seem strange that randomly guessing the candidate citation contexts would result in a better
recommended bibliography than the full-text baseline which
treats the entire full-text of a testing paper as a candidate
citation context. However, a testing document usually has
multiple citation needs (e.g. references to statistical models and references to optimization algorithms for fitting the
models) occurring in different places. Thus random guessing
can result in two distinct citation contexts (one relevant to
statistics and one relevant to optimization), each of which is
relevant to a document in D, instead of one giant candidate
citation context which is marginally related to many papers
(as in the case of full-text).
It is also interesting to note that although the topical relevance method considerably outperforms the random method
in finding citation locations, the bibliography that is subsequently generated is not much better than the method of
randomly guessing candidate citation contexts. One plausible reason is that the topical relevance method (and to some
extent the contextual similarity method and language models) are more susceptible to the redundancy mentioned in
Section 4.4. That is, one theme of a paper highly relevant
to other documents can recur many times, resulting in many
similar (thus redundant) candidate citation contexts being
chosen because of their high relevance score (meaning that
citation contexts representing a different theme are likely
to be left out). Random guessing on the other hand is not
affected by such redundancy issues.
Additional evidence supporting this claim is that the dependency feature model (Section 4.4) produces a considerably better bibliography by incorporating long-range dependency features. Thus if a window is highly relevant to documents from the corpus D, but is also very similar to a
previous window that was labeled as a candidate citation

context, the new windows is less likely to also be (redundantly) labeled as a candidate citation context.

6.

CONCLUSIONS

In this paper, we tackled the novel problem of automatically generating candidate citation contexts from the query
manuscript to aid an academic literature search. Our approach used existing citation contexts as succinct descriptions of the contributions and ideas present in documents
belonging to a large corpus. We are incorporating this approach into citation context identification and citation recommendation in CiteSeerX and we conducted extensive experiments to evaluate the performance of our approach.

7.

REFERENCES

[1] S. Aya, C. Lagoze, and T. Joachims. Citation
classification and its applications. In ICKM, 2005.
[2] C. Basu, H. Hirsh, W. Cohen, and C. Nevill-Manning.
Technical paper recommendation: A study in
combining multiple information sources. J. of
Artificial Intelligence Research, 2001.
[3] D. Blei, A. Ng, and M. Jordan. Latent dirichlet
allocation. J. Machine Learning Research, 2003.
[4] A. Broder, M. Fontoura, V. Josifovski, and L. Riedel.
A semantic approach to contextual advertising. In
SIGIR, 2007.
[5] K. Chandrasekaran, S. Gauch, P. Lakkaraju, and
H. Luong. Concept-Based Document Recommendations
for CiteSeer Authors. Adaptive Hypermedia and
Adaptive Web-Based Systems, Springer, 2008.
[6] D. Cohn and T. Hofmann. The missing link - a
probabilistic model of document content and
hypertext connectivity. In NIPS, 2001.
[7] R. Durrett. Probability: Theory and Examples.
Duxbury Press, 2nd edition, 1995.
[8] E. Erosheva, S. Fienberg, and J. Lafferty. Mixed
membership models of scientific publications. PNAS,
2004.
[9] Q. He, J. Pei, D. Kifer, P. Mitra, and L. Giles.
Context-aware citation recommendation. In WWW,
2010.
[10] S. Huang, G. Xue, B. Zhang, Z. Chen, Y. Yu, and
W. Ma. Tssp: A reinforcement algorithm to find
related papers. In WI, 2004.
[11] S. Kataria, P. Mitra, and S. Bhatia. Utilizing context
in generative bayesian models for linked corpus. In
AAAI, 2010.
[12] S. M. Katz. Estimation of probabilities from sparse
data for the language model component of a speech
recogniser. IEEE Transactions on Acoustics, Speech,
and Signal Processing, 35(3):400–401, 1987.
[13] J. Kleinberg. Bursty and hierarchical structure in
streams. In SIGKDD, 2002.
[14] D. Liben-Nowell and J. Kleinberg. The link prediction
problem for social networks. In CIKM, 2003.
[15] S. McNee, I. Albert, D. Cosley, P. Gopalkrishnan,
S. Lam, A. Rashid, J. Konstan, and J. Riedl. On the
recommending of citations for research papers. In
CSCW, 2002.
[16] R. Nallapati, A. Ahmed, E. Xing, and W. Cohen.
Joint latent topic models for text and citations. In

1

0.049

0.9

0.044

0.6
0.5
0.4

0.034

0.18

0.029

0.16

NDCG

co-cited probability

recall

0.7

0.024

0.12
0.1

0.009
25

50

75

100

125

150

175

200

225

250

top k recommendations per document

(a) recall

0.14

0.019
0.014

0.3

Ground Truth
Dependency
Model
Topic Relevance
Random
Fulltext
Contextual
Similarity
Trigram LM
Bigram LM
Unigram LM

0.2

0.039

0.8

0.2

0.22

0.004

25

50

75

100 125 150 175 200 225 250

0.08

25

50

top k recommendations per document

75

100 125 150 175 200 225 250

top k recommendations per document

(b) co-cited probability

(c) NDCG

Figure 2: Compare performances for citation recommendation.

SIGKDD, 2008.
[17] J. M. Ponte and W. B. Croft. A language modeling
approach to information retrieval. In Research and
Development in Information Retrieval, 1998.
[18] J. R. Quinlan. C4.5: programs for machine learning.
Morgan Kaufmann, 1993.
[19] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and E. S.
de Moura. Impedance coupling in contenttargeted
advertising. In SIGIR, 2005.
[20] C. Rijsbergen. The Geometry of Information
Retrieval. Cambridge University Press, 2004.
[21] A. Ritchie. Citation context analysis for information
retrieval. PhD thesis, University of Cambridge, 2008.
[22] B. Shaparenko and T. Joachims. Identifying the
original contribution of a document via language
modeling. In ECML, 2009.
[23] D. Simon. Optimal State Estimation: Kalman, H
Infinity, and Nonlinear Approaches.
Wiley-Interscience, 2006.
[24] T. Strohman, B. Croft, and D. Jensen. Recommending
citations for academic papers. In SIGIR, 2007.
[25] J. Tang and J. Zhang. A discriminative approach to
topic-based citation recommendations. In PAKDD,
2009.
[26] R. Torres, S. McNee, M. Abel, J. Konstan, and
J. Riedl. Enhancing digitial libraries with techlens. In
JCDL, 2004.
[27] V. von Brzeski, U. Irmak, and R. Kraft. Leveraging
context in user-centric entity detection systems. In
CIKM, 2007.
[28] F. Wang, B. Chen, and Z. Miao. A survey on reviewer
assignment problem. In IEA/AIE, 2008.
[29] W. Yih, J. Goodman, and V. R. Carvalho. Finding
advertising keywords on web pages. In WWW, 2006.
[30] Y. Zhao and G. Karypis. Hierarchical clustering
algorithms for document datasets. Data Mining and
Knowledge Discovery, 10(2):141–168, 2005.
[31] D. Zhou, S. Zhu, K. Yu, X. Song, B. Tseng, H. Zha,
and L. Giles. Learning multiple graphs for document
recommendations. In WWW, 2008.

APPENDIX
A. PROOF OF LEMMA 4.1

Suppose the pi are generated independently by the beta
distribution Beta(α, β) with α = γp, β = γ(1−p). Beta(α, β)
α
= p and variance (α+β)2αβ
= p(1 −
has mean α+β
(α+β+1)
p)/(γ + 1). In Bayesian statistics, Beta(α + mi , β + ki − mi )
can be seen as the posterior distribution of the parameter pi of Binomial(pi , ki ) after seeing mi successes in ki
coin tosses. So, the expected value of Binomial(ki , pi ) is
E(mi ) = pki and its variance is V ar(mi ) = V ar(pi ki ) +
E[ki pi (1 − pi )] (by the law of total variance [7]), which is
given by
V ar(pi ki ) + E[ki pi (1 − pi )]
= ki2 V ar(pi ) + ki (E[pi ] − E[p2i ])
α+1
= ki2 V ar(pi ) + ki (E[pi ] −
E[pi ])
α+β+1


(γp + 1)
k2 p(1 − p)
= i
+ ki p −
·p
γ+1
γ+1
ki2 p(1 − p)
ki pγ + ki p − ki p2 γ − ki p
+
γ+1
γ+1
2
k p(1 − p)
γ
= i
+ ki p(1 − p)
γ+1
γ+1
ki p(1 − p)(ki + γ)
=
.
γ+1
=

Thus E[mi /ki ] = p and


p(1 − p)(ki + γ)
mi
1
V ar
= 2 V ar(mi ) =
.
ki
ki
ki (γ + 1)
It is easy to see that the covariance of mi /ki and mj /kj is
0 for i 6= j. Now, we are looking for the convex combination
a1 (m1 /k1 ) + · · · + an (mn /kn ) with minimum variance. This
is a simple optimization problem with lagrange multipliers
and the solution is that ai ∝ 1/V ar(mi /ki ) [23]. This gives

p̂ =

1
V ar(mi /ki )
1
j=1 V ar(mj /kj )

n
X

Pn
i=1

=

=

·

n
X

1

kj (γ+1)
j=1 p(1−p)(kj +γ) i=1

Pn

1

n
X

kj
j=1 kj +γ i=1

Pn

mi
.
ki + γ

mi
ki
mi (γ + 1)
p(1 − p)(ki + γ)
(4)

